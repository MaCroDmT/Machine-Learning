# -*- coding: utf-8 -*-
"""Flood Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B_irVEX3Dr-w1a3q10xLZlUvCZ9cJo-U

Headers
"""

from google.colab import drive
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_score

import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import LogisticRegression

from sklearn.ensemble import RandomForestClassifier

from sklearn.tree import DecisionTreeClassifier

from sklearn.svm import SVC

from sklearn.neighbors import KNeighborsClassifier

from xgboost import XGBClassifier

"""Dataset Mounting"""

drive.mount('/content/drive')
file_path = '/content/drive/My Drive/MLFT/_Malaysia+Flood+Dataset_Malaysia+Flood+Dataset.csv'

data = pd.read_csv(file_path)

"""Dataset Info"""

print("Dataset Preview:")
print(data.head())


print("\nDataset Info:")
print(data.info())


print("\nDataset Summary Statistics:")
print(data.describe())

"""Pre-Processing"""

data.rename(columns={'0V': 'NOV'}, inplace=True)

"""Removing Duplicates"""

duplicate_count = data.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_count}")

if duplicate_count > 0:
    data = data.drop_duplicates()
    print("Duplicates removed.")

"""Data Types"""

print("\nData Types:")
print(data.dtypes)

"""Heatmap of missing values"""

plt.figure(figsize=(10, 6))
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Values Heatmap")
plt.show()

"""Data Distributions"""

print(data.columns)

plt.figure(figsize=(8, 4))
data.columns = data.columns.str.strip()
sns.countplot(x='FLOOD', data=data, palette='coolwarm')
plt.title("Flood Occurrences Distribution")
plt.show()

plt.figure(figsize=(8, 4))
sns.histplot(data['ANNUAL RAINFALL'], kde=True, color='blue')
plt.title("Annual Rainfall Distribution")
plt.xlabel("Annual Rainfall (mm)")
plt.ylabel("Frequency")
plt.show()

flooded_data = data[data['FLOOD'] == 1]
non_flooded_data = data[data['FLOOD'] == 0]

months = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']
flooded_avg = flooded_data[months].mean()
non_flooded_avg = non_flooded_data[months].mean()

plt.figure(figsize=(10, 6))
plt.plot(months, flooded_avg, label='Flooded Years', marker='o', color='blue')
plt.plot(months, non_flooded_avg, label='Non-Flooded Years', marker='o', color='orange')
plt.title('Average Monthly Rainfall Trends (Flooded vs Non-Flooded Years)')
plt.xlabel('Months')
plt.ylabel('Average Rainfall (mm)')
plt.legend()
plt.grid(True)
plt.show()

"""Boxplot for annual rainfall vs flood occurrences"""

plt.figure(figsize=(8, 6))
sns.boxplot(x='FLOOD', y='ANNUAL RAINFALL', data=data, palette='coolwarm')
plt.title('Annual Rainfall Distribution (Flooded vs Non-Flooded Years)')
plt.xlabel('Flood Occurrence (0 = No Flood, 1 = Flood)')
plt.ylabel('Annual Rainfall (mm)')
plt.show()

"""Correlation matrix"""

plt.figure(figsize=(10, 8))
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='magma')
plt.title('Correlation Matrix')
plt.show()

"""Train-Test Split"""

X = data.drop('FLOOD', axis=1)
y = data['FLOOD']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Logistic Regression"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

logreg = LogisticRegression(max_iter=1000)

logreg.fit(X_train_scaled, y_train)

y_pred_logreg = logreg.predict(X_test_scaled)

print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_logreg))
print("Classification Report:\n", classification_report(y_test, y_pred_logreg))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_logreg))

cm = confusion_matrix(y_test, y_pred_logreg)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Flood', 'Flood'], yticklabels=['No Flood', 'Flood'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Random Forest Classifier"""

rf = RandomForestClassifier()

rf.fit(X_train_scaled, y_train)

y_pred_rf = rf.predict(X_test_scaled)

print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['No Flood', 'Flood'], yticklabels=['No Flood', 'Flood'])
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Decesion Tree Classifier"""

dt_clf = DecisionTreeClassifier(random_state=42)

dt_clf.fit(X_train_scaled, y_train)

y_pred_dt = dt_clf.predict(X_test_scaled)

print("Decision Tree Classifier Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Classification Report:\n", classification_report(y_test, y_pred_dt))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))

cm_dt = confusion_matrix(y_test, y_pred_dt)

plt.figure(figsize=(6, 4))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=['No Flood', 'Flood'], yticklabels=['No Flood', 'Flood'])
plt.title('Decision Tree Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Support Vector Machine (SVM)"""

svm_clf = SVC(kernel='rbf', random_state=42)

svm_clf.fit(X_train_scaled, y_train)

y_pred_svm = svm_clf.predict(X_test_scaled)

print("\nSupport Vector Machine Accuracy:", accuracy_score(y_test, y_pred_svm))
print("Classification Report:\n", classification_report(y_test, y_pred_svm))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))

cm_svm = confusion_matrix(y_test, y_pred_svm)

plt.figure(figsize=(6, 4))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Purples', xticklabels=['No Flood', 'Flood'], yticklabels=['No Flood', 'Flood'])
plt.title('Support Vector Machine - Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""K-Nearest Neighbors"""

knn_clf = KNeighborsClassifier(n_neighbors=5)

knn_clf.fit(X_train_scaled, y_train)

y_pred_knn = knn_clf.predict(X_test_scaled)

print("\nK-Nearest Neighbors Accuracy:", accuracy_score(y_test, y_pred_knn))
print("Classification Report:\n", classification_report(y_test, y_pred_knn))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))

cm_knn = confusion_matrix(y_test, y_pred_knn)

plt.figure(figsize=(6, 4))
sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Greens', xticklabels=['No Flood', 'Flood'], yticklabels=['No Flood', 'Flood'])
plt.title('K-Nearest Neighbors - Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Gradient Boosting model - XGBoost"""

xgb_clf = XGBClassifier(random_state=42, eval_metric='logloss')

xgb_clf.fit(X_train_scaled, y_train)

y_pred_xgb = xgb_clf.predict(X_test_scaled)

print("\nXGBoost Classifier Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Classification Report:\n", classification_report(y_test, y_pred_xgb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))

cm_xgb = confusion_matrix(y_test, y_pred_xgb)

plt.figure(figsize=(6, 4))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Oranges', xticklabels=['No Flood', 'Flood'], yticklabels=['No Flood', 'Flood'])
plt.title('XGBoost - Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Model Evaluations"""

model_results = {}

model_results['Logistic Regression'] = classification_report(y_test, y_pred_logreg, output_dict=True)

model_results['Random Forest'] = classification_report(y_test, y_pred_rf, output_dict=True)

model_results['Decision Tree'] = classification_report(y_test, y_pred_dt, output_dict=True)

model_results['SVM'] = classification_report(y_test, y_pred_svm, output_dict=True)

model_results['KNN'] = classification_report(y_test, y_pred_knn, output_dict=True)

model_results['XGBoost'] = classification_report(y_test, y_pred_xgb, output_dict=True)

evaluation_df = pd.DataFrame({
    'Accuracy': [accuracy_score(y_test, y_pred_logreg), accuracy_score(y_test, y_pred_rf), accuracy_score(y_test, y_pred_dt),
                 accuracy_score(y_test, y_pred_xgb), accuracy_score(y_test, y_pred_svm), accuracy_score(y_test, y_pred_knn)],
    'Precision (Flood)': [model_results['Logistic Regression']['1']['precision'], model_results['Random Forest']['1']['precision'],
                          model_results['Decision Tree']['1']['precision'], model_results['XGBoost']['1']['precision'],
                          model_results['SVM']['1']['precision'], model_results['KNN']['1']['precision']],
    'Recall (Flood)': [model_results['Logistic Regression']['1']['recall'], model_results['Random Forest']['1']['recall'],
                       model_results['Decision Tree']['1']['recall'], model_results['XGBoost']['1']['recall'],
                       model_results['SVM']['1']['recall'], model_results['KNN']['1']['recall']],
    'F1-score (Flood)': [model_results['Logistic Regression']['1']['f1-score'], model_results['Random Forest']['1']['f1-score'],
                         model_results['Decision Tree']['1']['f1-score'], model_results['XGBoost']['1']['f1-score'],
                         model_results['SVM']['1']['f1-score'], model_results['KNN']['1']['f1-score']],
})

evaluation_df.index = ['Logistic Regression', 'Random Forest', 'Decision Tree', 'SVM', 'KNN', 'XGBoost']
print(evaluation_df)

results = {
    'Accuracy': [],
    'Precision (Flood)': [],
    'Recall (Flood)': [],
    'F1-score (Flood)': []
}

models = [
    ('Logistic Regression', logreg),
    ('Random Forest', rf),
    ('Decision Tree', dt_clf),
    ('SVM', svm_clf),
    ('KNN', knn_clf),
    ('XGBoost', xgb_clf)
]

for model_name, model in models:
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results['Accuracy'].append(accuracy)
    results['Precision (Flood)'].append(precision)
    results['Recall (Flood)'].append(recall)
    results['F1-score (Flood)'].append(f1)

evaluation_df = pd.DataFrame(results, index=['Logistic Regression', 'Random Forest', 'Decision Tree', 'XGBoost', 'SVM', 'KNN'])

custom_colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#FFB74D', '#8C564B']
fig, ax = plt.subplots(figsize=(10, 6))

evaluation_df.plot(kind='bar', ax=ax, color=custom_colors)
ax.set_xlabel('Models')
ax.set_ylabel('Scores')
ax.set_title('Model Comparison by Evaluation Metrics')
ax.set_xticklabels(evaluation_df.index, rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Model Selection"""

evaluation_df['Best Model Ranking'] = evaluation_df['Accuracy'].rank(ascending=False) + \
                                        evaluation_df['Precision (Flood)'].rank(ascending=False) + \
                                        evaluation_df['Recall (Flood)'].rank(ascending=False) + \
                                        evaluation_df['F1-score (Flood)'].rank(ascending=False)

evaluation_df = evaluation_df.sort_values(by='Best Model Ranking')

print(evaluation_df[['Accuracy', 'Precision (Flood)', 'Recall (Flood)', 'F1-score (Flood)', 'Best Model Ranking']])

dt_feature_importance = dt_clf.feature_importances_
dt_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': dt_feature_importance
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=dt_importance_df)
plt.title('Decision Tree Feature Importances')
plt.show()

xgb_feature_importance = xgb_clf.feature_importances_
xgb_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': xgb_feature_importance
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=xgb_importance_df)
plt.title('XGBoost Feature Importances')
plt.show()

cv_scores = cross_val_score(dt_clf, X, y, cv=5, scoring='accuracy')

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cv_scores) + 1), cv_scores, marker='o', linestyle='-', color='b')
plt.title('Cross-Validation Scores for Decision Tree')
plt.xlabel('Fold Number')
plt.ylabel('Accuracy')
plt.show()


print(f"Mean CV accuracy: {cv_scores.mean():.4f}")
print(f"Standard Deviation of CV accuracy: {cv_scores.std():.4f}")

"""Checking for the Best model"""

rf_clf = RandomForestClassifier(random_state=42)

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(rf_clf, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best score: {grid_search.best_score_}")

best_model = grid_search.best_estimator_
print(f"The best model selected is: {best_model}")
joblib.dump(best_model, 'final_model.pkl')

loaded_model = joblib.load('final_model.pkl')
print(f"Loaded model is: {loaded_model}")